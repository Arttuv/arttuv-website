{"componentChunkName":"component---src-templates-blog-template-js","path":"/writings/being-coupled-to-the-dynamics-of-an-environment","result":{"data":{"markdownRemark":{"html":"<p>Expert group for EU has published guidelines to promote Trustworthy AI. By Trustworthy AI they mean an AI that is</p>\n<ul>\n<li>Lawful</li>\n<li>Ethical</li>\n<li>Robust</li>\n</ul>\n<p>The document doesn’t concentrate on the ”lawful” part - AI should follow the applicaple local laws and regulations. For ethicality they list both ethical principles and requirements that lean on those principles. They provide a framework that can be used to ensure that the end result is ethical. An assessment list is provided to be used for assessing the end result and guide development.</p>\n<p>I like the approach this expert group has taken towards AI. I’m not that interested in total autonomy right now, because it’s still quite far away and I think there is a lot of potential in augmenting humans and empowering them using AI. </p>\n<p>To give a short overview, here are the <strong>principles</strong> and <strong>requirements</strong> for Trustworthy AI.</p>\n<p><strong>Principles</strong></p>\n<ul>\n<li>Respect for human autonomy</li>\n<li>Prevention of harm</li>\n<li>Fairness and explicability</li>\n</ul>\n<p><strong>Requirements</strong></p>\n<ul>\n<li>Human agency and oversight</li>\n<li>Technical robustness and safety</li>\n<li>Privacy and data governance</li>\n<li>Transparency</li>\n<li>Diversity, non-discrimination and fairness</li>\n<li>Environmental and societal well-being</li>\n<li>Accountability</li>\n</ul>\n<p>From the human-computer interaction point of view I’d like to highlight ”respect for human autonomy” and corresponding requirement for ”human agency and oversight”. </p>\n<p>Document states that AI systems should be designed to ”augment, complement and empower human cognitive, social and cultural skills”. This viewpoint is human-centric aiming to keep humans relevant and their work meaningful. System should allow a meaningful opportunity for human choice securing also the human oversight.</p>\n<p>Sometimes it feels like this is the value that is easiest to forget. Laws and regulations (and even public attention) make sure you don’t create a system that causes harm or is unfair, but often it feels like nobody is thinking about the end users who will need to interact with the system and <em>do the tasks that the AI is not able to</em>. </p>\n<p>Oversimplifying the situation: </p>\n<p>Engineers want to see how novel AI systems they are able to create, and the procurer wants to as much as possible to get cost and time savings. This can lead in to a situation in which everything that can be automated, will be automated, and humans need to care of the incoherent collection of random tasks that are left behind. If you are not actively part of the work the system does, it’s hard to understand what is really happening and you start to lose the skills that are needed when the system fails.</p>\n<p>David Mindell has described this problematic in Our Robots, Our Selves comparing to solutions for performing landings in bad weather.</p>\n<p>Fully autonomous landing systems turn pilots into observers that may have a hard time understanding what the system is doing, and why. It may be hard for them to intervine if the system fails because they are out of the loop. On the other hand they may also lose the skills to do the landing manually.</p>\n<p>Heads up displays on the other hand give extra tools for the pilots to be used while they are handling the flying. Same tools can be used in good and in bad weather making the experience similar in each situation. This means they are learning all the time and landing in bad weather is not a completely different experience.</p>\n<p>I’m advocating this augmentation approach whenever it makes sense. Often keeping the human in the loop, or \"coupled to the dynamics of an environment\" as Stanton et al. describes it, is beneficial and also socially and ethically viable.</p>\n<p>There is also the concern on security which Andrew Rae has been considering: Who should we put the ultimate trust on? In general, I’d say that human users should always be able to override AI decisions (and in a meaningful way as well), but it’s not always that simple either.</p>\n<p>For human oversight the document lists three different approaches:</p>\n<ul>\n<li>Human-in-the-loop (HITL): Human intervention in every decision cycle</li>\n<li>Human-on-the-loop (HOTL): Monitoring  the system’s operation</li>\n<li>Human-in-command (HIC): Overseeing the overall activity of the AI system and deciding when and if and how to use the system in any situation</li>\n</ul>\n<p>I’d say that autonomous landing system would mean human-on-the-loop, HUD system human-in-the-loop and entity deciding which one to use and how it affects the pilots and safety would be human-in-command.</p>\n<p>How do you ensure human agency and make sure users are able to self-assess and challenge the system? If security is at all of importance and the work requires special skills that should be kept up to date: I’d say keep end users as much in-the-loop as possible.</p>\n<p>The title comes from State-of-Science: Situation Awareness in Individuals, Teams and Systems (Stanton, Neville A., Matthew Salmon, Walker Guy H., Hancock, Peter A.)</p>","frontmatter":{"date":"May 03, 2020","path":"/writings/being-coupled-to-the-dynamics-of-an-environment","title":"Augment, Complement and Empower Human Cognitive Skills","tags":["hci"],"summary":null,"featuredImage":{"childImageSharp":{"sizes":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAACvElEQVQ4y71UPWgUQRSeu2uiFp5ErCSoh5JKUhhERCFyWAiKKN5PVJJLcidmd3bvbu+SS0SCqUxsz0pBiYVCQBAEsbMTO7GzsFEQrCyN7u48vzd7u7erJqRy4DFv3rzvm/dm3hsh/segRkOQlIIsK1hbVgrrlNbZXq/reXtkDOg59/RMbC8THqIwq61IyTASp0JPRxFKmYXsTRDbdkQc+iWj6oNT0KOoACjB9gnyBTL5x4HpOJ5MU8SJRJwI65NYvyHHIdwp4d6IWi2C7S32zkR+wISZRVfAyo/ZYOGacj/Wj5ESMRki9ADwAfRhd6nZ1OTQn8H3UJRJSPbLlEwS6IbskI1InBZ5plQgcSE8U0w87Plh5MAsMxY2zSXoUUeQ3Uv7nnOUHPs51QFsWj4OUr7UxFpYZxvvUcNmn1e04owyljOk+21WL0Eu9wK/mBOisFY4OEEfyjcVtWzF5CDRgoOJ2rb6eH1WTRyepIworgMzHGCLAdfuVEkMiKIu2j3p0lJGlEiIKwS7mhup0LcZE49ha/leM2np2BQNZkqKfVLwzaZLqyFhFlx63BmdEisnprX+9GxN8ulpUWSQOrCjrB6MVelJvkpHdo1rG7Lwx3MVWstXF8JHWQaHHhsGcm/Z4QPdpU4bj2LT6ws33NP7rjEYERdUT+j44FX14lzNpQbSX2jzVXT1/d9qiJ/gwnNbwg9bzbKGQLrqSblBi20uDf/hWNXL7RxXQwNl6p6a8bDv8R4wPvQuMDn9yuBRXNixjujr0hpGOazrgl5s0ddpw/1cMVxaQKk4TS6VlyAc+Rc2aJnYr6Ji3YLaygP8jubQIfNtJnoP2/lYMfNnkYp4wuKmWi1xCvcookyHa9yTAWnS7fmQKOrjvyLc4pNI9Haid/sZBR/Ctv7EOCggyaiYbdOotiROkor4j73Z+A1LtQDY+4mAzQAAAABJRU5ErkJggg==","aspectRatio":1,"src":"/static/09a697aac38c3fdf566b9556adc130bb/62915/hci-placeholder.png","srcSet":"/static/09a697aac38c3fdf566b9556adc130bb/72176/hci-placeholder.png 63w,\n/static/09a697aac38c3fdf566b9556adc130bb/62d80/hci-placeholder.png 125w,\n/static/09a697aac38c3fdf566b9556adc130bb/62915/hci-placeholder.png 128w","sizes":"(max-width: 128px) 100vw, 128px"}}}}}},"pageContext":{}}}